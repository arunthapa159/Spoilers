{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Preprocessing and exploring the dataset (Finished).ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"HMSN47Bzr5KU","colab_type":"text"},"source":["# Preprocessing and exploring the dataset"]},{"cell_type":"code","metadata":{"id":"5LgTVyqTr5KW","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xb3nYLbzr5Kd","colab_type":"text"},"source":["## General information about the dataset\n","- records = 573913\n","- users = 263407\n","- movies = 1572\n","- spoiler reviews = 150924\n","- users with at least one spoiler review = 79039\n","- items with at least one spoiler review = 1570"]},{"cell_type":"markdown","metadata":{"id":"nCK5SK0Wr5Kf","colab_type":"text"},"source":["## Importing the movie reviews dataset\n","Importing the reviews dataset and showing the first five values.\n","More information about this dataset can be found in [this page](https://www.kaggle.com/rmisra/imdb-spoiler-dataset/)\n","\n","- **review_date:** Date the review was written.\n","- **movie_id:** Unique id for the item.\n","- **user_id:** Unique id for the review author.\n","- **is_spoiler:** Indication whether review contains a spoiler.\n","- **review_text:** Text review about the item.\n","- **rating:** Rating given by the user to the item.\n","- **review_summary:** Short summary of the review."]},{"cell_type":"code","metadata":{"id":"0Y78XqE3r5Kh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":306},"outputId":"749bedbb-eb26-44dd-8bff-ed172cf26d55","executionInfo":{"status":"error","timestamp":1574855619883,"user_tz":-60,"elapsed":3041,"user":{"displayName":"Stephan MÃ¼ller","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAvOmWkWEQnD94Gw83eKwYBWVRKdi7jyQPs0Rq3=s64","userId":"02774318687088041377"}}},"source":["df = pd.read_json('IMDB_reviews.json', lines=True)"],"execution_count":4,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-b173dbfe72f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'IMDB_reviews.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshould_close\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1093\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m             )\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected object or value"]}]},{"cell_type":"code","metadata":{"id":"oGmbdfOtr5Kk","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xRof6fuNr5Kp","colab_type":"code","colab":{}},"source":["print('Columns of reviews dataset:', df.columns)\n","print('\\nUser reviews shape: ', df.shape)\n","print('Unique films in reviews dataset:', df['movie_id'].nunique())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"uLA_heKyr5Kr","colab_type":"code","colab":{}},"source":["# We check if the dataframe contains null values\n","df.isnull().values.any()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nNJLxbm3r5Ku","colab_type":"text"},"source":["Since the dataset does not contain null values, we don't have to worry about missing values. We keep only the columns that we are going to use for classification (The review text and the target variable).\n","<br>\n","\n","We also convert the ```is_spoiler``` label to one hot encoding"]},{"cell_type":"code","metadata":{"id":"e-qKC1Utr5Kv","colab_type":"code","colab":{}},"source":["df = df[['is_spoiler', 'review_text']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WFqBMOv5r5Ky","colab_type":"code","colab":{}},"source":["df.is_spoiler = df.is_spoiler.astype(int)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hlM-fzSzr5K1","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mqPN1HsUr5K5","colab_type":"text"},"source":["## Checking if the dataset is balanced or not\n","We check if the data is balanced. The results indicate that the data is not balanced, so **we will have to balance the train set** (we can't never balance the dataset)"]},{"cell_type":"code","metadata":{"id":"dAGhM2PUr5K5","colab_type":"code","colab":{}},"source":["spoiler_length = len(df.loc[df['is_spoiler']==True])\n","not_spoiler_length = len(df.loc[df['is_spoiler']==False])\n","\n","spoiler_percentage = (spoiler_length*100)/(spoiler_length + not_spoiler_length)\n","not_spoiler_percentage = (not_spoiler_length*100)/(spoiler_length + not_spoiler_length)\n","\n","print('Number of reviews with spoilers: ' + str(spoiler_length) + ' (' + str(round(spoiler_percentage)) + '%)')\n","print('Number of reviews without spoilers: ' + str(not_spoiler_length) + ' (' + str(round(not_spoiler_percentage)) + '%)')\n","\n","# Graphical Representation\n","labels = 'Spoiler', 'Not spoiler'\n","explode = (0.1, 0)\n","plt.pie([spoiler_length, not_spoiler_length], explode=explode, labels = labels,autopct='%1.1f%%', shadow=True, startangle=90)\n","plt.title('Pie chart Spoiler vs Not-Spoiler')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T2AN-3-7r5K8","colab_type":"text"},"source":["## Balancing the dataset\n","In the current dataset, there is a 26% of reviews with spoilers and a 74% of reviews without spoilers, so we have to use some technique to balance the data (if we don't do this, the resulting algorithm would have good accuracy in majority class samples, but almost 0 accuracy in minority class samples). <br>\n","We considered using Imbalanced-learn techniques (example: imblearn), but we had to discard the idea due to memory problems (the computers of the team members ran out of memory when trying to calculate the matrices). <br>\n","We are going to use resampling to balance the dataset, there are two possible ways of applying resampling:\n","- Oversampling: Duplicate random records of minority class (Problem: Possible overfitting)\n","- Undersampling: Eliminate samples from the majority class (Problem: Possible Loss of information)\n","\n","<br>\n","The dataset is considered relatively big (422.989 reviews without spoilers and 150.924 reviews with spoilers), so we can eliminate samples from the majority class without facing a relevant loss of information. That's why we determined that using undersampling is the best option."]},{"cell_type":"code","metadata":{"id":"J3dVgqppr5K9","colab_type":"code","colab":{}},"source":["# Class count\n","count_not_spoiler, count_spoiler = df.is_spoiler.value_counts()\n","\n","# Divide by class\n","df_spoiler = df[ df['is_spoiler'] == 1 ]\n","df_not_spoiler = df[ df['is_spoiler'] == 0 ]\n","\n","# Random undersampling\n","# We reduce the number of not spoiler to the number of spoiler\n","# We use floor division (//)\n","df_not_spoiler_under = df_not_spoiler.sample(int(count_spoiler//2))\n","df_spoiler_under = df_spoiler.sample(int(count_spoiler//2))\n","df_test_under = pd.concat([df_not_spoiler_under, df_spoiler_under], axis=0)\n","\n","\n","# The resulting dataset is balanced (100.616 reviews with spoilers and 100.616 without spoilers)\n","print('Random Undersampling')\n","df_test_under.is_spoiler.value_counts().plot(kind='bar', title='Count')\n","plt.show()\n","print(df_test_under.is_spoiler.value_counts())"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vswjKf5Zr5K_","colab_type":"text"},"source":["After balancing the dataset, we obtain a subset with 75.462 reviews containing spoilers and the same number of reviews without spoilers. In this case, there is no majority class, so we could use 0.5 accuracy as baseline"]},{"cell_type":"markdown","metadata":{"id":"_zyE-J0Yr5LA","colab_type":"text"},"source":["## Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"R1oaatFhr5LB","colab_type":"text"},"source":["We follow the following process for preprocessing:\n","1. **Lowercase:** We convert the text to lowercase to avoid repetition of words\n","2. **Remove html tags, email addresses and urls**\n","3. **Remove punctuation symbols**\n","4. **POS Tagging:** Necessary for Lemmatization, since it takes the category of the word as a parameters, otherwise the lemmatizer function considers the word as a noun.\n","5. **Lemmatization:** Receives the type of word as a parameter (obtained after POS tagging). We also tried to apply stemming (both Lancaster and Porter). But it returned worse results.\n","6. **Stop Words Removal:** We didn't apply this before because we needed the stopwords to apply POS tagging correctly\n","\n","While exploring the dataset, we also found some words in foreign languages, such as japanese, korean, chinese and russian. Nevertheless, there were only 30 japanese words in total, and less than 10 of the other types, so we could consider these words as noise. In each method, this will be solved in a different way (applying methods such as min_df filtering)."]},{"cell_type":"code","metadata":{"id":"lx2Rre10r5LC","colab_type":"code","colab":{}},"source":["# We create a map that takes the outputs of pos-tagging and convert them into the inputs of lemmatization\n","# We use name as the default value\n","pos_map = {\n","'CC': 'n','CD': 'n', 'DT': 'n','EX': 'n', 'FW': 'n','IN': 'n', 'JJ': 'a','JJR': 'a', 'JJS': 'a','LS': 'n', 'MD': 'v','NN': 'n',\n","'NNS': 'n','NNP': 'n', 'NNPS': 'n','PDT': 'n', 'POS': 'n','PRP': 'n', 'PRP$': 'r','RB': 'r', 'RBR': 'r','RBS': 'r', 'RP': 'n','TO': 'n',\n","'UH': 'n','VB': 'v', 'VBD': 'v','VBG': 'v', 'VBN': 'v','VBP': 'v', 'VBZ': 'v','WDT': 'n', 'WP': 'n','WP$': 'n', 'WRB': 'r'\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yJyEKW1r5LE","colab_type":"code","colab":{}},"source":["# Importing stop words\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","stop_words = set(stopwords.words('english'))\n","len(stop_words)\n","stop_words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"ynumCXq9r5LG","colab_type":"code","colab":{}},"source":["from nltk.stem import WordNetLemmatizer\n","\n","example_sent = \"<HTML>This <p>is.a</p> ! jua@email.com sentences, showing off the <br> stop words filtration. http://www.youtube.com\"\n","# example_sent = \"Hi, it's me\"\n","# Since all the stopwords are in lower case, we have to convert the string to lowercase first\n","example_sent = example_sent.lower()\n","\n","# This was a simple tokenizer that kept the punctuation symbols\n","# word_tokens = word_tokenize(example_sent)\n","\n","# Removing url, emails and html tags\n","# HTML TAGS\n","from bs4 import BeautifulSoup\n","example_sent = BeautifulSoup(example_sent, 'lxml').text\n","\n","# EMAIL ADDRESSES\n","import re\n","example_sent = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', example_sent)\n","\n","# URLs\n","example_sent = re.sub(r'http\\S+', '', example_sent)\n","\n","# Removing punctuation symbol\n","from nltk.tokenize import RegexpTokenizer\n","tokenizer = RegexpTokenizer(r'\\w+')\n","word_tokens = tokenizer.tokenize(example_sent)\n","# Now we have obtained the tokenized words without punctuation symbols and with stopwords\n","\n","# POS Tagging the data (the stopwords improve the accuracy of the pos tagging, so we'll remove them later)\n","# This method returns a list of tuples: (word, classification)\n","tags = nltk.pos_tag(word_tokens)\n","print('tags', tags)\n","\n","lemmatizer = WordNetLemmatizer()\n","# We lemmatize all the words in the text by their category\n","for i, word in enumerate(word_tokens):\n","    # Returns the lemmatized word given its category (if the key is not part of the map, the word is considered a noun)\n","    word_tokens[i] = lemmatizer.lemmatize(word, pos=pos_map.get(tags[i][1] , 'n'))\n","\n","# Removing stop words\n","filtered_sentence = [w for w in word_tokens if not w in stop_words]\n","\n","# Japanese words are kept as a single word, so we can remove them easily, but urls, emails and html tags are splitted, so we\n","# have to remove them before tokenizing\n","\n","# In html:  <br>  -->  br\n","# In email:  jua@gmail.com  --> jua, gmail, com\n","# In url: https://www.youtube.com  --> http, www, youtube, com\n","filtered_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WyoT8su-r5LJ","colab_type":"text"},"source":["### Return values of POS Tagging\n","\n","- **CC:**\tcoordinating conjunction\n","- **CD:**\tcardinal digit\n","- **DT:**\tdeterminer\n","- **EX:**\texistential there (like: \"there is\" ... think of it like \"there exists\")\n","- **FW:**\tforeign word\n","- **IN:**\tpreposition/subordinating conjunction\n","- **JJ:**\tadjective\t'big'\n","- **JJR:**\tadjective, comparative\t'bigger'\n","- **JJS:**\tadjective, superlative\t'biggest'\n","- **LS:**\tlist marker\t1)\n","- **MD:**\tmodal\tcould, will\n","- **NN:**\tnoun, singular 'desk'\n","- **NNS:**\tnoun plural\t'desks'\n","- **NNP:**\tproper noun, singular\t'Harrison'\n","- **NNPS:**\tproper noun, plural\t'Americans'\n","- **PDT:**\tpredeterminer\t'all the kids'\n","- **POS:** possessive ending\tparent\\'s\n","- **PRP:**\tpersonal pronoun\tI, he, she\n","- **PRP\\$:**\tpossessive pronoun\tmy, his, hers\n","- **RB:**\tadverb\tvery, silently,\n","- **RBR:**\tadverb, comparative\tbetter\n","- **RBS:**\tadverb, superlative\tbest\n","- **RP:**\tparticle\tgive up\n","- **TO:**\tto\tgo 'to' the store.\n","- **UH:**\tinterjection\terrrrrrrrm\n","- **VB:**\tverb, base form\ttake\n","- **VBD:**\tverb, past tense\ttook\n","- **VBG:**\tverb, gerund/present participle\ttaking\n","- **VBN:**\tverb, past participle\ttaken\n","- **VBP:**\tverb, sing. present, non-3d\ttake\n","- **VBZ:**\tverb, 3rd person sing. present\ttakes\n","- **WDT:**\twh-determiner\twhich\n","- **WP:**\twh-pronoun\twho, what\n","- **WP\\$:**\tpossessive wh-pronoun\twhose\n","- **WRB:**\twh-abverb\twhere, when\n","\n","### Possible values of pos in lemmatization\n","ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'"]},{"cell_type":"markdown","metadata":{"id":"vipdLJxTr5LJ","colab_type":"text"},"source":["We have to download the stopwords: <br>\n","```>>> import nltk ``` <br>\n","```>>> nltk.download('stopwords') ``` <br><br>\n","We have to download wordnet for lemmatization<br>\n","```>>> import nltk ``` <br>\n","```>>> nltk.download('wordnet')``` <br><br>\n","We have to download for POS-Tagging <br>\n","```>>> import nltk``` <br>\n","```>>> nltk.download('averaged_perceptron_tagger')```"]},{"cell_type":"markdown","metadata":{"id":"0GUWVFF9r5LK","colab_type":"text"},"source":["# Converting the previous preprocessing into a function\n","This is the function that we are going to apply in the next notebooks for preprocessing"]},{"cell_type":"code","metadata":{"id":"Qiu0HHqOr5LL","colab_type":"code","colab":{}},"source":["from bs4 import BeautifulSoup\n","import re\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.stem import WordNetLemmatizer\n","\n","# We initialize the lemmatizer\n","lemmatizer = WordNetLemmatizer()\n","\n","def tokenizer(example_sent):\n","    # example_sent = \"Hi, it's me\"\n","    # Since all the stopwords are in lower case, we have to convert the string to lowercase first\n","    example_sent = example_sent.lower()\n","\n","    # This was a simple tokenizer that kept the punctuation symbols\n","    # word_tokens = word_tokenize(example_sent)\n","    \n","    # Japanese words are kept as a single word, so we can remove them easily, but urls, emails and html tags are splitted, so we\n","    # have to remove them before tokenizing\n","    \n","    # Removing url, emails and html tags\n","    # HTML TAGS\n","    example_sent = BeautifulSoup(example_sent, 'lxml').text\n","\n","    # EMAIL ADDRESSES\n","    example_sent = re.sub(r'[\\w\\.-]+@[\\w\\.-]+', ' ', example_sent)\n","\n","    # URLs\n","    example_sent = re.sub(r'http\\S+', '', example_sent)\n","\n","    # Removing punctuation symbol\n","    tokenizer = RegexpTokenizer(r'\\w+')\n","    word_tokens = tokenizer.tokenize(example_sent)\n","    # Now we have obtained the tokenized words without punctuation symbols and with stopwords\n","\n","    # POS Tagging the data (the stopwords improve the accuracy of the pos tagging, so we'll remove them later)\n","    # This method returns a list of tuples: (word, classification)\n","    tags = nltk.pos_tag(word_tokens)\n","\n","    # We lemmatize all the words in the text by their category\n","    for i, word in enumerate(word_tokens):\n","        # Returns the lemmatized word given its category (if the key is not part of the map, the word is considered a noun)\n","        word_tokens[i] = lemmatizer.lemmatize(word, pos=pos_map.get(tags[i][1] , 'n'))\n","\n","    # Removing stop words\n","    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n","\n","    # In html:  <br>  -->  br\n","    # In email:  jua@gmail.com  --> jua, gmail, com\n","    # In url: https://www.youtube.com  --> http, www, youtube, com\n","    return filtered_sentence"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JB7eoLBDr5LN","colab_type":"code","colab":{}},"source":["test = \"<HTML>This <p>is.a</p> ! jua@email.com sentences, showing off the <br> stop words filtration. http://www.youtube.com\"\n","tokenizer(test)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G6JEAtjgr5LQ","colab_type":"text"},"source":["Applying the function to the dataset"]},{"cell_type":"code","metadata":{"id":"w-qSaZFsr5LQ","colab_type":"code","colab":{}},"source":["%%time\n","df['review_text'] = df['review_text'].apply(tokenizer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-ht7ihyEr5LT","colab_type":"code","colab":{}},"source":["df.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dwj3Wwx7r5LX","colab_type":"code","colab":{}},"source":["reviews_pie = pd.DataFrame()\n","reviews_pie['is_spoiler'] = df['is_spoiler']\n","reviews_pie['has_word_twist'] = df['review_text'].apply(lambda text: 1 if 'twist' in text else 0)\n","reviews_pie['has_word_end'] = df['review_text'].apply(lambda text: 1 if 'end' in text else 0)\n","reviews_pie['has_word_spoiler'] = df['review_text'].apply(lambda text: 1 if 'spoiler' in text else 0)\n","reviews_pie['has_word_die'] = df['review_text'].apply(lambda text: 1 if 'die' in text else 0)\n","reviews_pie['has_word_death'] = df['review_text'].apply(lambda text: 1 if 'death' in text else 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0MKYg_Xr5Lc","colab_type":"code","colab":{}},"source":["reviews_pie.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"4UnIp9sGr5Lf","colab_type":"code","colab":{}},"source":["pie1 = reviews_pie['is_spoiler'].value_counts().reset_index().sort_values(by='index')\n","pie2 = reviews_pie[reviews_pie['has_word_twist'] == 1]['is_spoiler'].value_counts().reset_index().sort_values(by='index')\n","pie3 = reviews_pie[reviews_pie['has_word_end'] == 1]['is_spoiler'].value_counts().reset_index().sort_values(by='index')\n","pie4 = reviews_pie[reviews_pie['has_word_spoiler'] == 1]['is_spoiler'].value_counts().reset_index().sort_values(by='index')\n","pie5 = reviews_pie[reviews_pie['has_word_die'] == 1]['is_spoiler'].value_counts().reset_index().sort_values(by='index')\n","pie6 = reviews_pie[reviews_pie['has_word_death'] == 1]['is_spoiler'].value_counts().reset_index().sort_values(by='index')\n","\n","with plt.style.context('seaborn-talk'):\n","    fig = plt.figure(figsize=(16, 16))\n","\n","    ax1 = fig.add_subplot(3, 2, 1)\n","    ax2 = fig.add_subplot(3, 2, 2)\n","    ax3 = fig.add_subplot(3, 2, 3)\n","    ax4 = fig.add_subplot(3, 2, 4)\n","    ax5 = fig.add_subplot(3, 2, 5)\n","    ax6 = fig.add_subplot(3, 2, 6)\n","\n","    ax1.pie(pie1['is_spoiler'])\n","    ax1.set_title('All reviews')\n","\n","    ax2.pie(pie2['is_spoiler'])\n","    ax2.set_title('Reviews containing the word \\'twist\\'')\n","\n","    ax3.pie(pie3['is_spoiler'])\n","    ax3.set_title('Reviews containing the word \\'end\\'')\n","\n","    ax4.pie(pie4['is_spoiler'])\n","    ax4.set_title('Reviews containing the word \\'spoiler\\'')\n","    \n","    ax5.pie(pie4['is_spoiler'])\n","    ax5.set_title('Reviews containing the word \\'die\\'')\n","    \n","    ax6.pie(pie4['is_spoiler'])\n","    ax6.set_title('Reviews containing the word \\'death\\'')\n","\n","    plt.suptitle('Spoiler distribution within the reviews', fontsize=20)\n","    fig.legend(labels=['Without spoilers', 'With spoilers'], loc='center')\n","\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ncob-mUDr5Lj","colab_type":"text"},"source":["These graphics prove that it's possible to find patterns in the data"]}]}